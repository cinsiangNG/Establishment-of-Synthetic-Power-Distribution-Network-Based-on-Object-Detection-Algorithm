{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQeVbtqaFJ2e",
    "outputId": "6b4f4d21-57e2-451b-975f-55c16c29b8ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GfwlsTWmFu_9",
    "outputId": "a9d8e538-7142-447f-b05d-6dcb0d35448c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 2.8.0+cu126 _CudaDeviceProperties(name='NVIDIA A100-SXM4-80GB', major=8, minor=0, total_memory=81221MB, multi_processor_count=108, uuid=ad05a8ee-fedd-7862-940b-41b0a52f4b69, pci_bus_id=0, pci_device_id=5, pci_domain_id=0, L2_cache_size=40MB)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print('torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "biZHQfsgF1RB",
    "outputId": "45a30b95-95e2-4019-ad5d-c2493e5d0af3"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       " function ClickConnect(){\n",
       "   btn = document.querySelector(\"colab-connect-button\")\n",
       "   if (btn != null){\n",
       "     console.log(\"Click colab-connect-button\");\n",
       "     btn.click()\n",
       "     }\n",
       "\n",
       "   btn = document.getElementById('ok')\n",
       "   if (btn != null){\n",
       "     console.log(\"Click reconnect\");\n",
       "     btn.click()\n",
       "     }\n",
       "  }\n",
       "\n",
       "setInterval(ClickConnect,60000)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "from google.colab import output\n",
    "\n",
    "display(IPython.display.Javascript('''\n",
    " function ClickConnect(){\n",
    "   btn = document.querySelector(\"colab-connect-button\")\n",
    "   if (btn != null){\n",
    "     console.log(\"Click colab-connect-button\");\n",
    "     btn.click()\n",
    "     }\n",
    "\n",
    "   btn = document.getElementById('ok')\n",
    "   if (btn != null){\n",
    "     console.log(\"Click reconnect\");\n",
    "     btn.click()\n",
    "     }\n",
    "  }\n",
    "\n",
    "setInterval(ClickConnect,60000)\n",
    "'''))\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eRNprHTHIkXz",
    "outputId": "2f377945-2021-43b4-d437-8f9f21c04315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content/drive/MyDrive/colab/detect/yolov5'\n",
      "/content\n",
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/colab/detect/yolov5\n",
    "!pip install -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HV4Hh9rUH6k6",
    "outputId": "b329649f-7e1e-479a-a842-24cf4c13f3c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/ÈªÉ‰øäÁøî_Ë´ñÊñáÁ†îÁ©∂üìö/Street_view/Streetview from junior/detect\n",
      "Requirement already satisfied: streetlevel in /usr/local/lib/python3.12/dist-packages (0.12.4)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from streetlevel) (3.13.1)\n",
      "Requirement already satisfied: bd09convertor in /usr/local/lib/python3.12/dist-packages (from streetlevel) (0.0.3)\n",
      "Requirement already satisfied: coordinatesconverter in /usr/local/lib/python3.12/dist-packages (from streetlevel) (0.1.5)\n",
      "Requirement already satisfied: cython in /usr/local/lib/python3.12/dist-packages (from streetlevel) (3.0.12)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from streetlevel) (2.3.4)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from streetlevel) (11.3.0)\n",
      "Requirement already satisfied: protobuf>4 in /usr/local/lib/python3.12/dist-packages (from streetlevel) (5.29.5)\n",
      "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.12/dist-packages (from streetlevel) (3.23.0)\n",
      "Requirement already satisfied: pyequilib in /usr/local/lib/python3.12/dist-packages (from streetlevel) (0.5.8)\n",
      "Requirement already satisfied: pyexiv2 in /usr/local/lib/python3.12/dist-packages (from streetlevel) (2.15.5)\n",
      "Requirement already satisfied: pyfrpc in /usr/local/lib/python3.12/dist-packages (from streetlevel) (0.2.13)\n",
      "Requirement already satisfied: pyproj in /usr/local/lib/python3.12/dist-packages (from streetlevel) (3.7.2)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.12/dist-packages (from streetlevel) (8.4.2)\n",
      "Requirement already satisfied: pytest-asyncio in /usr/local/lib/python3.12/dist-packages (from streetlevel) (1.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from streetlevel) (2.32.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from streetlevel) (1.16.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->streetlevel) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->streetlevel) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->streetlevel) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->streetlevel) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->streetlevel) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->streetlevel) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->streetlevel) (1.22.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from pyproj->streetlevel) (2025.10.5)\n",
      "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest->streetlevel) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from pytest->streetlevel) (25.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest->streetlevel) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest->streetlevel) (2.19.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from pytest-asyncio->streetlevel) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->streetlevel) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->streetlevel) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->streetlevel) (2.5.0)\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/ÈªÉ‰øäÁøî_Ë´ñÊñáÁ†îÁ©∂üìö/Street_view/Streetview from junior/detect\n",
    "!pip install streetlevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQJpbvX6TApk"
   },
   "outputs": [],
   "source": [
    "# prepare required functions\n",
    "import math\n",
    "\n",
    "def decode_polyline(polyline_str):\n",
    "    '''Pass a Google Maps encoded polyline string; returns list of lat/lon pairs'''\n",
    "    index, lat, lng = 0, 0, 0\n",
    "    coordinates = []\n",
    "    changes = {'latitude': 0, 'longitude': 0}\n",
    "\n",
    "    # Coordinates have variable length when encoded, so just keep\n",
    "    # track of whether we've hit the end of the string. In each\n",
    "    # while loop iteration, a single coordinate is decoded.\n",
    "    while index < len(polyline_str):\n",
    "        # Gather lat/lon changes, store them in a dictionary to apply them later\n",
    "        for unit in ['latitude', 'longitude']:\n",
    "            shift, result = 0, 0\n",
    "            while True:\n",
    "                byte = ord(polyline_str[index]) - 63\n",
    "                index += 1\n",
    "                result |= (byte & 0x1f) << shift\n",
    "                shift += 5\n",
    "                if not byte >= 0x20:\n",
    "                    break\n",
    "            if (result & 1):\n",
    "                changes[unit] = ~(result >> 1)\n",
    "            else:\n",
    "                changes[unit] = (result >> 1)\n",
    "        lat += changes['latitude']\n",
    "        lng += changes['longitude']\n",
    "        coordinates.append((lat / 100000.0, lng / 100000.0))\n",
    "    return coordinates\n",
    "\n",
    "def getpathpoints(dirs,pts_distance=50):\n",
    "  # polypath = dirs[0]['legs'][0]['steps'][0]['polyline']['points']\n",
    "  polypath = dirs[0]['overview_polyline']['points']\n",
    "  polypath_pts = decode_polyline(polypath)\n",
    "  pts_coords = []\n",
    "  for pt in range(1, len(polypath_pts)):\n",
    "    lat1 = polypath_pts[pt-1][0]\n",
    "    lng1 = polypath_pts[pt-1][1]\n",
    "    lat2 = polypath_pts[pt][0]\n",
    "    lng2 = polypath_pts[pt][1]\n",
    "    azimuth = calculateBearing(lat1,lng1,lat2,lng2)\n",
    "    coords = getInnerPoints(pts_distance,azimuth,lat1,lng1,lat2,lng2)\n",
    "    if pt==1:\n",
    "      pts_coords.append(coords)\n",
    "    else:\n",
    "      pts_coords.append(coords[1:])\n",
    "  flatten_pts_coords = []\n",
    "  for subl in pts_coords:\n",
    "      for item in subl:\n",
    "          flatten_pts_coords.append(item)\n",
    "  return flatten_pts_coords\n",
    "\n",
    "def getPathLength(lat1,lng1,lat2,lng2):\n",
    "    '''calculates the distance between two lat, long coordinate pairs'''\n",
    "    R = 6371000 # radius of earth in m\n",
    "    lat1rads = math.radians(lat1)\n",
    "    lat2rads = math.radians(lat2)\n",
    "    deltaLat = math.radians((lat2-lat1))\n",
    "    deltaLng = math.radians((lng2-lng1))\n",
    "    a = math.sin(deltaLat/2) * math.sin(deltaLat/2) + math.cos(lat1rads) * math.cos(lat2rads) * math.sin(deltaLng/2) * math.sin(deltaLng/2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    d = R * c\n",
    "    return d\n",
    "\n",
    "def getDestinationLatLong(lat,lng,azimuth,distance):\n",
    "    '''returns the lat an long of destination point\n",
    "    given the start lat, long, aziuth, and distance'''\n",
    "    R = 6378.1 #Radius of the Earth in km\n",
    "    brng = math.radians(azimuth) #Bearing is degrees converted to radians.\n",
    "    d = distance/1000 #Distance m converted to km\n",
    "    lat1 = math.radians(lat) #Current dd lat point converted to radians\n",
    "    lon1 = math.radians(lng) #Current dd long point converted to radians\n",
    "    lat2 = math.asin(math.sin(lat1) * math.cos(d/R) + math.cos(lat1)* math.sin(d/R)* math.cos(brng))\n",
    "    lon2 = lon1 + math.atan2(math.sin(brng) * math.sin(d/R)* math.cos(lat1), math.cos(d/R)- math.sin(lat1)* math.sin(lat2))\n",
    "    #convert back to degrees\n",
    "    lat2 = math.degrees(lat2)\n",
    "    lon2 = math.degrees(lon2)\n",
    "    return[lat2, lon2]\n",
    "\n",
    "def calculateBearing(lat1,lng1,lat2,lng2):\n",
    "    '''calculates the azimuth in degrees from start point to end point'''\n",
    "    startLat = math.radians(lat1)\n",
    "    startLong = math.radians(lng1)\n",
    "    endLat = math.radians(lat2)\n",
    "    endLong = math.radians(lng2)\n",
    "    dLong = endLong - startLong\n",
    "    dPhi = math.log(math.tan(endLat/2.0+math.pi/4.0)/math.tan(startLat/2.0+math.pi/4.0))\n",
    "    if abs(dLong) > math.pi:\n",
    "         if dLong > 0.0:\n",
    "             dLong = -(2.0 * math.pi - dLong)\n",
    "         else:\n",
    "             dLong = (2.0 * math.pi + dLong)\n",
    "    bearing = (math.degrees(math.atan2(dLong, dPhi)) + 360.0) % 360.0;\n",
    "    return bearing\n",
    "\n",
    "def getInnerPoints(interval,azimuth,lat1,lng1,lat2,lng2):\n",
    "    '''returns every coordinate pair inbetween two coordinate\n",
    "    pairs given the desired interval'''\n",
    "\n",
    "    d = getPathLength(lat1,lng1,lat2,lng2)\n",
    "    remainder, dist = math.modf((d / interval))\n",
    "    counter = float(interval)\n",
    "    coords = []\n",
    "    coords.append([lat1,lng1])\n",
    "    for distance in range(0,int(dist)):\n",
    "        coord = getDestinationLatLong(lat1,lng1,azimuth,counter)\n",
    "        counter = counter + float(interval)\n",
    "        coords.append(coord)\n",
    "    coords.append([lat2,lng2])\n",
    "    return coords\n",
    "\n",
    "def samepoint(line1, line2):\n",
    "  result = False\n",
    "  for x in line1:\n",
    "    for y in line2:\n",
    "      if x == y:\n",
    "        result = True\n",
    "        return x\n",
    "        break\n",
    "  return result\n",
    "\n",
    "def find_intersection(p0, p1, p2, p3):\n",
    "    # line1 = [p0.tolist(),p1.tolist()]\n",
    "    # line2 = [p2.tolist(),p3.tolist()]\n",
    "    line1 = [p0,p1]\n",
    "    line2 = [p2,p3]\n",
    "    samept = samepoint(line1,line2)\n",
    "    if samept is not False:\n",
    "      intersection_point = samept\n",
    "    else:\n",
    "      s10_x = p1[0] - p0[0]\n",
    "      s10_y = p1[1] - p0[1]\n",
    "      s32_x = p3[0] - p2[0]\n",
    "      s32_y = p3[1] - p2[1]\n",
    "      denom = s10_x * s32_y - s32_x * s10_y\n",
    "      if denom == 0 : return math.nan # collinear\n",
    "      denom_is_positive = denom > 0\n",
    "      s02_x = p0[0] - p2[0]\n",
    "      s02_y = p0[1] - p2[1]\n",
    "      s_numer = s10_x * s02_y - s10_y * s02_x\n",
    "      if (s_numer < 0) == denom_is_positive : return math.nan # no collision\n",
    "      t_numer = s32_x * s02_y - s32_y * s02_x\n",
    "      if (t_numer < 0) == denom_is_positive : return math.nan # no collision\n",
    "      if (s_numer > denom) == denom_is_positive or (t_numer > denom) == denom_is_positive : return math.nan # no collision\n",
    "      # collision detected\n",
    "      t = t_numer / denom\n",
    "      intersection_point = [ p0[0] + (t * s10_x), p0[1] + (t * s10_y) ]\n",
    "    return intersection_point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n1gZQWhHRTh7"
   },
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# plot points on map\n",
    "# pt1 = pd.DataFrame(Pts2downloadPano,columns=['lat','lon'])\n",
    "# gdf = geopandas.GeoDataFrame(pt1, crs = {'init': 'epsg:4326'}, geometry=geopandas.points_from_xy(pt1.lon, pt1.lat)).to_crs(epsg=3857)\n",
    "# gdf = geopandas.GeoDataFrame(pt1, geometry=geopandas.points_from_xy(pt1.lon, pt1.lat))\n",
    "\n",
    "mapbox_access_token = \"pk.eyJ1IjoiY2hpeWluZ2xpbiIsImEiOiJja24xNDB3eWkwaWh1Mm5vMnRud2cwdTZ1In0.mVM5aPE2TgbxeKo1il1CUQ\"\n",
    "\n",
    "## using plotly ##\n",
    "def mapplotpoints(lat,lon,size=12,zoom=16,color='blue',name=None,text=None):\n",
    "  # input gdf in ndarrays\n",
    "  fig = go.Figure(go.Scattermapbox(\n",
    "          lat= list(lat),\n",
    "          lon= list(lon),\n",
    "          mode='markers',\n",
    "          name=name,\n",
    "          marker=go.scattermapbox.Marker(\n",
    "              size=size,\n",
    "              color=color # named CSS color\n",
    "          ),\n",
    "          text=text\n",
    "      ))\n",
    "  fig.update_layout(\n",
    "      autosize=True,\n",
    "      hovermode='closest',\n",
    "      mapbox=dict(\n",
    "          accesstoken=mapbox_access_token,\n",
    "          bearing=0,\n",
    "          center=go.layout.mapbox.Center(\n",
    "              # lat= np.mean(gdf['lat'].values),\n",
    "              # lon= np.mean(gdf['lon'].values),\n",
    "              lat= np.mean(lat),\n",
    "              lon= np.mean(lon),\n",
    "          ),\n",
    "          pitch=0,\n",
    "          zoom=zoom\n",
    "      )\n",
    "  )\n",
    "  #fig.show()\n",
    "  return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQAdqWAWkeNs"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from aiohttp import ClientSession\n",
    "from streetlevel import streetview\n",
    "import os\n",
    "import math\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "# Resize the downloaded image to 640x640 pixels\n",
    "def resize_image(input_path, output_path, size):\n",
    "    with Image.open(input_path) as img:\n",
    "        img_resized = img.resize(size)\n",
    "        img_resized.save(output_path)\n",
    "\n",
    "# Asynchronous function to download and resize a street view image\n",
    "async def download_and_resize_street_view(lat, lon, path1, path2, size):\n",
    "    # Ensure the target folder exists\n",
    "    if not os.path.exists(path1):\n",
    "        os.makedirs(path1)\n",
    "    if not os.path.exists(path2):\n",
    "        os.makedirs(path2)\n",
    "\n",
    "    async with ClientSession() as session:\n",
    "        # Find the panorama\n",
    "        pano = await streetview.find_panorama_async(lat, lon, session)\n",
    "        # Define the file path for the original image\n",
    "        original_image_path = os.path.join(path1, f\"{pano.id}.jpg\")\n",
    "        # Check if the image already exists\n",
    "        if os.path.exists(original_image_path):\n",
    "            print(f\"Image {pano.id}.jpg already exists. Using the existing image.\")\n",
    "        else:\n",
    "            # Download the panorama image and save it\n",
    "            await streetview.download_panorama_async(pano, original_image_path, session)\n",
    "        # Resize the image after downloading\n",
    "        resized_path = os.path.join(path2, f\"{pano.id}resized.jpg\")\n",
    "        resize_image(original_image_path, resized_path, size)\n",
    "        # Store the heading in degrees\n",
    "        heading = pano.heading * 180 / math.pi\n",
    "\n",
    "    return heading, resized_path\n",
    "\n",
    "# Clean directory\n",
    "def clean_directory(directory):\n",
    "    if os.path.exists(directory):\n",
    "        for file in os.listdir(directory):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            try:\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.unlink(file_path)\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "        print(f\"Successfully cleaned up the directory: {directory}\")\n",
    "    else:\n",
    "        print(f\"The directory does not exist: {directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RThwM16TY8o"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import gridspec\n",
    "import io\n",
    "\n",
    "#### Bounding boxes info extraction\n",
    "def BBInfoExtraction(allbox,imgwidth,coor,dist_para,heading):\n",
    "  # parameters for distance inferecing\n",
    "  pole_Height = dist_para[0] # 40*304.8 set standard pole as 40 ft height above ground (mm)\n",
    "  camera_fl = dist_para[1] # 1.5 camera focal length (mm)\n",
    "  camera_sh = dist_para[2] # 4.55 camera sensor height (mm)\n",
    "  searchdist = dist_para[3] # 10 distance coverage from the proposed pole distance (m)\n",
    "  # extract pole info in a pano\n",
    "  bearings = []\n",
    "  LOB = []\n",
    "  for bb in allbox:\n",
    "    # bb = [x1, y1, x2, y2]\n",
    "    dtheta = (bb[0]+bb[2])/2-imgwidth/2\n",
    "    if dtheta > 0:\n",
    "      bearing = dtheta/imgwidth*360\n",
    "    else:\n",
    "      bearing = 360 + dtheta/imgwidth*360\n",
    "    bearing = (bearing + heading) %360\n",
    "    bearings.append(bearing)\n",
    "\n",
    "    pole_px = bb[3]-bb[1] # pixel height of detected pole bounding box\n",
    "    if pole_px > 0:\n",
    "      pole_dist = camera_fl*pole_Height*imageshape[0]/camera_sh/pole_px/1000 # calculated pole distance from camera (m)\n",
    "      # compute LOB based on the proposed pole location +/- a search distance\n",
    "      LOBstart = getDestinationLatLong(coor[0], coor[1], bearing, max(0,pole_dist-searchdist))\n",
    "      LOBend = getDestinationLatLong(coor[0], coor[1], bearing, min(80,pole_dist+searchdist))\n",
    "      LOB.append([LOBstart[0],LOBstart[1],LOBend[0],LOBend[1]])\n",
    "    else:\n",
    "      LOB.append([])\n",
    "  return LOB, bearings\n",
    "\n",
    "#### find intersections of predicted line of bearings (LOBs)\n",
    "def LOBintersections(poleLOB, startpoint, nvg = 4, plotresult=False):\n",
    "  def unnestlist(alist):\n",
    "    unnestedlist = []\n",
    "    for subl in alist:\n",
    "      for item in subl:\n",
    "        unnestedlist.append(item)\n",
    "    return unnestedlist\n",
    "\n",
    "  allintxn = []\n",
    "  for gg in range(0,len(poleLOB)-nvg+1):\n",
    "    subLOB = poleLOB[gg:gg+nvg] # subset of LOB group\n",
    "    intxninsub = [] # all intersections in this subgroup\n",
    "    for sg in range(0,nvg-1):\n",
    "      vc = subLOB[sg] # current LOB group\n",
    "      vo_temp = [x for i,x in enumerate(subLOB) if i!=sg] # all other LOB groups\n",
    "      vo = unnestlist(vo_temp) # all other LOBs\n",
    "      for i in range(0,len(vc)):\n",
    "        for j in range(0,len(vo)):\n",
    "          intersect = find_intersection(vc[i][0:2], vc[i][2:], vo[j][0:2], vo[j][2:])\n",
    "          if not intersect is math.nan:\n",
    "            intxninsub.append(intersect)\n",
    "    # a[gg] = intxninsub\n",
    "    allintxn.append(intxninsub)\n",
    "\n",
    "  # extract unique intersections\n",
    "  intxn_unnest = unnestlist(allintxn)\n",
    "  intxn_unique = [i for n, i in enumerate(intxn_unnest) if i not in intxn_unnest[:n]]\n",
    "  intxn_array = np.array(intxn_unique)\n",
    "\n",
    "  ## first grouping: combine intersection generated by prediction error\n",
    "  distMax = 1\n",
    "  intxngroup = []\n",
    "  for i in range(0,len(intxn_array)):\n",
    "    if i not in unnestlist(intxngroup):\n",
    "      pt0 = intxn_array[i]\n",
    "      dist = [(lambda x: getPathLength(pt0[0],pt0[1],x[0], x[1]))(x) for x in intxn_array]\n",
    "      closepts = [i for i, x in enumerate(np.array(dist) < distMax) if x]\n",
    "      intxngroup.append(closepts)\n",
    "\n",
    "  intxns = []\n",
    "  for i in range(0,len(intxngroup)):\n",
    "    lats = intxn_array[intxngroup[i],0]\n",
    "    lons = intxn_array[intxngroup[i],1]\n",
    "    intxns.append([np.mean(lats), np.mean(lons)])\n",
    "\n",
    "  ## second grouping: combine intersections very close to each other\n",
    "  distMax = 5\n",
    "  intxngroup = []\n",
    "\n",
    "  intxn_array2 = np.array(intxns)\n",
    "  for i in range(0,len(intxn_array2)):\n",
    "    if i not in unnestlist(intxngroup):\n",
    "      pt0 = intxn_array2[i]\n",
    "      dist = [(lambda x: getPathLength(pt0[0],pt0[1],x[0], x[1]))(x) for x in intxn_array2]\n",
    "      closepts = [i for i, x in enumerate(np.array(dist) < distMax) if x]\n",
    "      intxngroup.append(closepts)\n",
    "\n",
    "  intxns2 = []\n",
    "  for i in range(0,len(intxngroup)):\n",
    "    lats = intxn_array2[intxngroup[i],0]\n",
    "    lons = intxn_array2[intxngroup[i],1]\n",
    "    intxns2.append([np.mean(lats), np.mean(lons)])\n",
    "\n",
    "  ## check pole distance\n",
    "  maxpoledist = 70 # maximum allowable distance b/t poles [urban: 125 ft (38 m); rural: 300 ft (91 m)]\n",
    "  maxsegdist = 500 # maximum distance b/t route segments; output route points might be quite far away and belong to different road\n",
    "  intxns_sorted = []\n",
    "  intxns_index = []\n",
    "  dist = [(lambda x: getPathLength(startpoint[0],startpoint[1],x[0],x[1]))(x) for x in intxns2]\n",
    "  if dist:\n",
    "    intxns_index.append(np.argmin(dist))\n",
    "    intxns_sorted.append(intxns2[np.argmin(dist)])\n",
    "\n",
    "  for i in range(1,len(intxns2)):\n",
    "    prevpt = intxns_sorted[i-1]\n",
    "    dist = np.array([[n, (lambda x: getPathLength(prevpt[0],prevpt[1],x[0],x[1]))(x)] for n, x in enumerate(intxns2) if n not in intxns_index])\n",
    "    sortdist = dist[np.argsort(dist[:, 1])]\n",
    "    intxns_index.append(sortdist[0,0])\n",
    "    intxns_sorted.append(intxns2[int(sortdist[0,0])])\n",
    "\n",
    "  intxns3 = intxns_sorted.copy()\n",
    "  for i in range(1,len(intxns_sorted)-1):\n",
    "    pt0 = intxns_sorted[i]\n",
    "    pt1 = intxns_sorted[i+1]\n",
    "    dist2next = getPathLength(pt0[0],pt0[1],pt1[0],pt1[1])\n",
    "    if maxpoledist < dist2next < maxsegdist:\n",
    "      n_pole_needed = int(dist2next // maxpoledist) # number of pole needed to insert\n",
    "      dist2insertpole = dist2next/(n_pole_needed+1)\n",
    "      bearing = calculateBearing(pt0[0],pt0[1],pt1[0],pt1[1])\n",
    "      for j in range(0,n_pole_needed):\n",
    "        intxns3.append(getDestinationLatLong(pt0[0],pt0[1],bearing,dist2insertpole*(j+1)))\n",
    "  intxns_final = intxns3.copy()\n",
    "\n",
    "  ## plot results\n",
    "  if plotresult:\n",
    "    finalintxns = intxns_final\n",
    "    print('Number of intersections = {}'.format(len(finalintxns)))\n",
    "    pointtext=[str(x) for x in PanoInfoFinal.index+1]\n",
    "\n",
    "    fig = mapplotpoints(PanoInfoFinal['lat'],PanoInfoFinal['lon'],size = 8, name='street view location', color='DodgerBlue',text=pointtext)\n",
    "    fig.add_trace(go.Scattermapbox(\n",
    "            lat=np.array(finalintxns)[:,0],\n",
    "            lon=np.array(finalintxns)[:,1],\n",
    "            name='possible pole locations',\n",
    "            mode='markers',\n",
    "            marker=go.scattermapbox.Marker(\n",
    "                size=10,\n",
    "                color='red',\n",
    "                opacity=0.9\n",
    "            )))\n",
    "\n",
    "    for i in range(0,len(poleLOB)):\n",
    "      lob = poleLOB[i]\n",
    "      for j in range(0,len(lob)):\n",
    "        fig.add_trace(\n",
    "            go.Scattermapbox(\n",
    "                lon = [np.array(poleLOB[i][j])[1], np.array(poleLOB[i][j])[3]],\n",
    "                lat = [np.array(poleLOB[i][j])[0], np.array(poleLOB[i][j])[2]],\n",
    "                mode = 'lines',\n",
    "                line = dict(width = 1,color = 'Dodgerblue'),\n",
    "                opacity=0.5\n",
    "            )\n",
    "        )\n",
    "    fig.update_layout(\n",
    "        height=650\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "  return intxns_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_IuTh710XlOV",
    "outputId": "c29218b7-7a11-43c1-e8ef-8e9accb5882c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33321 entries, 0 to 33320\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   RTE_NM  33321 non-null  object \n",
      " 1   SEG_ID  33321 non-null  object \n",
      " 2   Lon     33321 non-null  float64\n",
      " 3   Lat     33321 non-null  float64\n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# read ROI route points (locations to download street view images)\n",
    "\n",
    "workingdirectory = '/content/'\n",
    "\n",
    "GalvestonRoutesPoints = pd.read_excel(workingdirectory + \"galveston_route_point.xlsx\",converters={'SEG_ID':str})\n",
    "RouteIDs = GalvestonRoutesPoints['RTE_NM'].unique() # unique route ID\n",
    "GalvestonRoutesPoints.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pgrLzTBjruYf",
    "outputId": "5acc239d-6475-4c6a-f180-538911e1a8ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.3.4)\n",
      "/content/drive/MyDrive/ÈªÉ‰øäÁøî_Ë´ñÊñáÁ†îÁ©∂üìö/Street_view/Streetview from junior/detect/yolov5\n",
      "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numpy\n",
    "%cd /content/drive/MyDrive/ÈªÉ‰øäÁøî_Ë´ñÊñáÁ†îÁ©∂üìö/Street_view/Streetview from junior/detect/yolov5\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# PATCH torch.load to use weights_only=False\n",
    "_original_torch_load = torch.load\n",
    "def _patched_torch_load(f, *args, **kwargs):\n",
    "    kwargs.setdefault('weights_only', False)\n",
    "    return _original_torch_load(f, *args, **kwargs)\n",
    "torch.load = _patched_torch_load\n",
    "\n",
    "from detect import run\n",
    "\n",
    "# ÊåáÂÆöÊñá‰ª∂Â§πÂêçÁß∞\n",
    "folder_name = \"/content/PolePredictionResults\"\n",
    "\n",
    "# ÂàõÂª∫Êñá‰ª∂Â§πÔºàÂ¶ÇÊûúÊñá‰ª∂Â§πÂ∑≤Â≠òÂú®Âàô‰∏ç‰ºöÊä•ÈîôÔºâ\n",
    "os.makedirs(folder_name, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ovldyzpX9y5",
    "outputId": "800e0e70-2c00-4df4-a9c6-4838b18d418b"
   },
   "outputs": [],
   "source": "import numpy as np\nimport json\nimport time\nimport os\nimport pickle\n\ndist_para = [40*304.8,1.5,4.55,10]\nsize = (640, 640)\n\n# Specify the folder to save images\npath1 = workingdirectory + 'StreetViewImages/'\npath2 = workingdirectory + 'resized/'\n\nsavepreddir = workingdirectory + 'PolePredictionResults/'\n\n# Checkpoint file to save progress (saved to Google Drive so it persists across sessions)\ncheckpoint_file = '/content/drive/MyDrive/ÈªÉ‰øäÁøî_Ë´ñÊñáÁ†îÁ©∂üìö/Street_view/checkpoint_progress.pkl'\n\n# Load checkpoint if exists\nstart_route = 0\ntotaldownload = 0\nfailed_downloads = 0\nfailed_requests = 0\n\nif os.path.exists(checkpoint_file):\n    try:\n        with open(checkpoint_file, 'rb') as f:\n            checkpoint = pickle.load(f)\n            start_route = checkpoint.get('last_completed_route', 0) + 1\n            totaldownload = checkpoint.get('totaldownload', 0)\n            failed_downloads = checkpoint.get('failed_downloads', 0)\n            failed_requests = checkpoint.get('failed_requests', 0)\n        print(f\"üìç Resuming from route {start_route + 1}/{len(RouteIDs)}\")\n        print(f\"   Previous stats: downloads={totaldownload}, failed_downloads={failed_downloads}, failed_requests={failed_requests}\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Warning: Could not load checkpoint: {e}\")\n        print(\"   Starting from beginning...\")\n        start_route = 0\nelse:\n    print(f\"üöÄ Starting fresh processing of {len(RouteIDs)} routes\")\n\nclean_directory('/content/drive/MyDrive/Ë´ñÊñáÁ†îÁ©∂üìö/Street_view/Streetview from junior/detect/yolov5/runs/detect')\n\nfor rr in range(start_route, len(RouteIDs)):\n  routeid = RouteIDs[rr]\n  routepoints = GalvestonRoutesPoints.loc[GalvestonRoutesPoints['RTE_NM'] == routeid]\n  Pts2downloadPano = routepoints[['Lat','Lon']].values.tolist()\n\n  allpanoid = []\n  for panoLatLon in Pts2downloadPano:\n    try:\n      pano = streetview.find_panorama(lat=panoLatLon[0], lon=panoLatLon[1])\n      if bool(pano):\n        panoid = pano.id\n        panolat = pano.lat\n        panolon = pano.lon\n        allpanoid.append([panoid, panolat, panolon])\n    except json.JSONDecodeError as e:\n      failed_requests += 1\n      print(f'\\nJSONDecodeError at location ({panoLatLon[0]}, {panoLatLon[1]}): {str(e)}')\n      print(f'Skipping this location and continuing... (Total failed requests: {failed_requests})')\n      time.sleep(1)  # Add a small delay to avoid rate limiting\n      continue\n    except Exception as e:\n      failed_requests += 1\n      print(f'\\nUnexpected error at location ({panoLatLon[0]}, {panoLatLon[1]}): {str(e)}')\n      print(f'Skipping this location and continuing... (Total failed requests: {failed_requests})')\n      time.sleep(1)\n      continue\n  PanoInfoFinal = pd.DataFrame(allpanoid,columns=['panoid','lat','lon']).drop_duplicates()\n\n  intersections = []\n\n  if len(PanoInfoFinal) > 3:\n    poleview = []\n    poleLOB = []\n\n    for pp in PanoInfoFinal.index:\n      print('\\r','Progress: route = {}/{}; pano = {}/{}; download = {}; failed downloads = {}; failed requests = {}'.format(\n          rr+1, len(RouteIDs), pp+1, len(PanoInfoFinal.index), totaldownload, failed_downloads, failed_requests), end = '')\n\n      lat = PanoInfoFinal['lat'][pp]\n      lon = PanoInfoFinal['lon'][pp]\n      pano_coor = [PanoInfoFinal['lat'][pp], PanoInfoFinal['lon'][pp]]\n\n      try:\n        # Try to download and process the image\n        heading, source = await download_and_resize_street_view(lat, lon, path1, path2, size)\n        totaldownload = totaldownload + 1\n\n        weights = '/content/drive/MyDrive/ÈªÉ‰øäÁøî_Ë´ñÊñáÁ†îÁ©∂üìö/Street_view/Streetview from junior/detect/yolov5/runs/train/exp_J/best.pt'\n\n        bounding_boxes = run(weights, source)\n        image = Image.open(source)\n        imageshape = np.shape(image)\n        clean_directory(path2)\n\n        # extract prediction and compute line of bearings\n        LOB, bearings = BBInfoExtraction(bounding_boxes,imageshape[1],pano_coor,dist_para,heading)\n        poleview.append([pano_coor, bearings])\n        poleLOB.append(LOB)\n\n      except Exception as e:\n        # Skip corrupted/failed images and continue\n        failed_downloads += 1\n        print(f\"\\nWarning: Failed to process image at lat={lat}, lon={lon}. Error: {str(e)}\")\n        clean_directory(path2)  # Clean up any partial files\n        continue\n\n    startpoint = [PanoInfoFinal['lat'][0], PanoInfoFinal['lon'][0]]\n    intersections = LOBintersections(poleLOB, startpoint)\n\n  if intersections:\n    poleLocations = pd.DataFrame(intersections,columns=['lat','lon'])\n    poleLocations.to_excel(savepreddir + 'Pred_r{:0>4d}.xlsx'.format(rr+1), index = False)\n\n  # Save checkpoint after each route (every 10 routes to reduce I/O)\n  if (rr + 1) % 10 == 0 or rr == len(RouteIDs) - 1:\n    try:\n      checkpoint = {\n          'last_completed_route': rr,\n          'totaldownload': totaldownload,\n          'failed_downloads': failed_downloads,\n          'failed_requests': failed_requests,\n          'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n      }\n      with open(checkpoint_file, 'wb') as f:\n          pickle.dump(checkpoint, f)\n      print(f'\\nüíæ Checkpoint saved at route {rr+1}/{len(RouteIDs)}')\n    except Exception as e:\n      print(f'\\n‚ö†Ô∏è  Warning: Could not save checkpoint: {e}')\n\nprint(f\"\\n\\n‚úÖ Processing complete!\")\nprint(f\"Total successful downloads: {totaldownload}\")\nprint(f\"Total failed downloads: {failed_downloads}\")\nprint(f\"Total failed API requests: {failed_requests}\")\n\n# Delete checkpoint file when done\nif os.path.exists(checkpoint_file):\n    os.remove(checkpoint_file)\n    print(f\"üóëÔ∏è  Checkpoint file deleted (processing complete)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß CheckpointÁÆ°ÁêÜÂ∑•ÂÖ∑ - Âú®ÈÅãË°å‰∏ªÁ®ãÂ∫èÂâç‰ΩøÁî®\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "checkpoint_file = '/content/drive/MyDrive/ÈªÉ‰øäÁøî_Ë´ñÊñáÁ†îÁ©∂üìö/Street_view/checkpoint_progress.pkl'\n",
    "\n",
    "# Ê™¢Êü•Áï∂ÂâçÈÄ≤Â∫¶\n",
    "if os.path.exists(checkpoint_file):\n",
    "    with open(checkpoint_file, 'rb') as f:\n",
    "        checkpoint = pickle.load(f)\n",
    "    print(\"üìä Áï∂ÂâçÈÄ≤Â∫¶:\")\n",
    "    print(f\"   ‰∏äÊ¨°ÂÆåÊàêÁöÑ route: {checkpoint['last_completed_route'] + 1}/{len(RouteIDs)}\")\n",
    "    print(f\"   ÊàêÂäü‰∏ãËºâÊï∏: {checkpoint['totaldownload']}\")\n",
    "    print(f\"   ‰∏ãËºâÂ§±ÊïóÊï∏: {checkpoint['failed_downloads']}\")\n",
    "    print(f\"   API Ë´ãÊ±ÇÂ§±ÊïóÊï∏: {checkpoint['failed_requests']}\")\n",
    "    print(f\"   ‰øùÂ≠òÊôÇÈñì: {checkpoint['timestamp']}\")\n",
    "    print(f\"\\n   Ââ©È§ò routes: {len(RouteIDs) - checkpoint['last_completed_route'] - 1}\")\n",
    "    print(f\"   ÂÆåÊàêÁôæÂàÜÊØî: {(checkpoint['last_completed_route'] + 1) / len(RouteIDs) * 100:.1f}%\")\n",
    "    print(\"\\n‚ö†Ô∏è  Â¶ÇÊûúË¶ÅÂæûÈ†≠ÈñãÂßãÔºåË´ãÈÅãË°å‰∏ãÈù¢ÁöÑÈáçÁΩÆ‰ª£Á¢º\")\n",
    "else:\n",
    "    print(\"üìç Ê≤íÊúâÊâæÂà∞ checkpoint Êñá‰ª∂\")\n",
    "    print(\"   Á®ãÂ∫èÂ∞áÂæûÈ†≠ÈñãÂßãÈÅãË°å\")\n",
    "\n",
    "# Â¶ÇÊûúË¶ÅÈáçÁΩÆÈÄ≤Â∫¶ÔºåÂèñÊ∂à‰∏ãÈù¢ÈÄôË°åÁöÑÊ≥®Èáã‰∏¶ÈÅãË°å:\n",
    "# if os.path.exists(checkpoint_file): os.remove(checkpoint_file); print(\"‚úÖ Checkpoint Â∑≤Âà™Èô§ÔºåÂ∞áÂæûÈ†≠ÈñãÂßã\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7Z_RJUUz3oKR"
   },
   "outputs": [],
   "source": [
    "print(Pts2downloadPano)\n",
    "print(PanoInfoFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "fEOxMlqjKD0t"
   },
   "outputs": [],
   "source": [
    "# combine all predicted locations\n",
    "# savepreddir = '/content/drive/MyDrive/ÈªÉ‰øäÁøî_Ë´ñÊñáÁ†îÁ©∂üìö/Street_view/Streetview from junior/detect/PolePredictionResults/'\n",
    "PredResultList = os.listdir(savepreddir)\n",
    "\n",
    "allprediction = []\n",
    "\n",
    "def unnestlist(alist):\n",
    "  unnestedlist = []\n",
    "  for subl in alist:\n",
    "    for item in subl:\n",
    "      unnestedlist.append(item)\n",
    "  return unnestedlist\n",
    "\n",
    "for i in range(0,len(PredResultList)):\n",
    "# for i in range(0,20):\n",
    "  pred_temp = pd.read_excel(savepreddir + PredResultList[i])\n",
    "  allprediction.append(pred_temp.values.tolist())\n",
    "  # if len(pred_temp) > 10:\n",
    "  #   allprediction.append(pred_temp.values.tolist())\n",
    "\n",
    "\n",
    "allprediction_T = pd.DataFrame(unnestlist(allprediction),columns=['Lat','Lon'])\n",
    "allprediction_T.to_excel(workingdirectory + '/drive/MyDrive/ÈªÉ‰øäÁøî_Ë´ñÊñáÁ†îÁ©∂üìö/Street_view/' + 'Galveston_test.xlsx', index = False)\n",
    "\n",
    "allprediction_T\n",
    "\n",
    "fig = mapplotpoints(allprediction_T['Lat'],allprediction_T['Lon'], size = 7, name='Predicted pole locations', color='orange', zoom = 15)\n",
    "fig.update_layout(\n",
    "    height=1200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "id": "z6qlaCJVcq5y",
    "outputId": "d8612d08-7ed4-440c-8175-e0104b6fd9b8"
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Destination path '/content/drive/MyDrive/ÈªÉ‰øäÁøî_Ë´ñÊñáÁ†îÁ©∂üìö/Street_view/StreetViewImages' already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-191919876139>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# ÁßªÂä®Êñá‰ª∂Â§π\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Destination path '%s' already exists\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Destination path '/content/drive/MyDrive/ÈªÉ‰øäÁøî_Ë´ñÊñáÁ†îÁ©∂üìö/Street_view/StreetViewImages' already exists"
     ]
    }
   ],
   "source": [
    "# import shutil\n",
    "# import os\n",
    "\n",
    "# # ÂÆö‰πâÊ∫êÊñá‰ª∂Â§πÂíåÁõÆÊ†áË∑ØÂæÑ\n",
    "# source_folder = '/content/StreetViewImages'\n",
    "# destination_path = '/content/drive/MyDrive/ÈªÉ‰øäÁøî_Ë´ñÊñáÁ†îÁ©∂üìö/Street_view'\n",
    "\n",
    "# # Á°Æ‰øùÁõÆÊ†áÊñá‰ª∂Â§πÂ≠òÂú®ÔºåÂ¶ÇÊûú‰∏çÂ≠òÂú®ÂàôÂàõÂª∫\n",
    "# if not os.path.exists(destination_path):\n",
    "#     os.makedirs(destination_path)\n",
    "\n",
    "# # ÁßªÂä®Êñá‰ª∂Â§π\n",
    "# shutil.move(source_folder, destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmYjvvSKK31A"
   },
   "outputs": [],
   "source": [
    "# compute accuracy\n",
    "# PctAcc1: Percentage of the number of actual poles being within a buffer zone of a predicted pole (%)\n",
    "# PctAcc2: Percentage of the number of predicted of poles being within a certain buffer zone of actual poles (%)\n",
    "\n",
    "WHPoleLoc = pd.read_excel(workingdirectory + \"PoleLocationGT.xlsx\",sheet_name='pole') # actual pole locations\n",
    "WHPolePred = pd.read_excel(workingdirectory + \"AllPredictions.xlsx\") # predicted pole locations\n",
    "\n",
    "distMax = 2\n",
    "polepredgroup = []\n",
    "\n",
    "polepred = np.array(WHPolePred)\n",
    "for i in range(0,len(polepred)):\n",
    "  if i not in unnestlist(polepredgroup):\n",
    "    pt0 = polepred[i]\n",
    "    dist = [(lambda x: getPathLength(pt0[0],pt0[1],x[0], x[1]))(x) for x in polepred]\n",
    "    closepts = [i for i, x in enumerate(np.array(dist) < distMax) if x]\n",
    "    polepredgroup.append(closepts)\n",
    "\n",
    "polepredgroup_final = []\n",
    "for i in range(0,len(polepredgroup)):\n",
    "  lats = polepred[polepredgroup[i],0]\n",
    "  lons = polepred[polepredgroup[i],1]\n",
    "  polepredgroup_final.append([np.mean(lats), np.mean(lons)])\n",
    "\n",
    "bufferdist = [1,2,3,5,7,10,15] # buffer distance (meter)\n",
    "\n",
    "PctAcc1 =[]\n",
    "PctAcc2 =[]\n",
    "\n",
    "for bb in range(0,len(bufferdist)):\n",
    "  poles_in_Buffer1 = 0\n",
    "  for i in range(0,len(polepredgroup_final)):\n",
    "    pt0 = np.array(polepredgroup_final[i]) # current pole\n",
    "    dist = [(lambda x: getPathLength(pt0[0],pt0[1],x[0], x[1]))(x) for x in WHPoleLoc.values.tolist()]\n",
    "    if min(dist) < bufferdist[bb]:\n",
    "      poles_in_Buffer1 += 1\n",
    "  PctAcc1.append(poles_in_Buffer1/len(polepredgroup_final))\n",
    "\n",
    "  poles_in_Buffer2 = 0\n",
    "  for i in range(0,len(WHPoleLoc)):\n",
    "    pt0 = np.array(WHPoleLoc.loc[i]) # current pole\n",
    "    dist = [(lambda x: getPathLength(pt0[0],pt0[1],x[0], x[1]))(x) for x in polepredgroup_final]\n",
    "    if min(dist) < bufferdist[bb]:\n",
    "      poles_in_Buffer2 += 1\n",
    "  PctAcc2.append(poles_in_Buffer2/len(WHPoleLoc))\n",
    "\n",
    "# PRDresult = pd.DataFrame(np.array([np.array(bufferdist),np.array(PctAcc1)]).T,columns=['Buffer(m)','PDR'])\n",
    "# PRDresult\n",
    "\n",
    "PRDresult = pd.DataFrame(np.array([np.array(bufferdist),np.array(PctAcc1),np.array(PctAcc2)]).T,columns=['Buffer(m)','PDR1','PDR2'])\n",
    "PRDresult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oOdCfrRYh_B"
   },
   "source": [
    "# **Save the predicted coordinate into excel file and export it to Google Drive.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGgyqqMMKN6Q"
   },
   "source": [
    "# **Compare with Ground truth dataset(For verifying)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dyccerBUQlhc"
   },
   "outputs": [],
   "source": [
    "fig = mapplotpoints(PanoInfoFinal['lat'][0:1],PanoInfoFinal['lon'][0:1],size = 1, name='Street view location', color='DodgerBlue')\n",
    "\n",
    "# fig = mapplotpoints(np.array(WHPoleLoc)[:,0],np.array(WHPoleLoc)[:,1],size = 8, name='Actual pole locations', color='orange')\n",
    "\n",
    "fig.add_trace(go.Scattermapbox(\n",
    "        lat=np.array(WHPoleLoc)[:,0],\n",
    "        lon=np.array(WHPoleLoc)[:,1],\n",
    "        name='Actual pole locations',\n",
    "        mode='markers',\n",
    "        marker=go.scattermapbox.Marker(\n",
    "            size=15,\n",
    "            color='orange',\n",
    "            opacity=0.6\n",
    "        )))\n",
    "\n",
    "fig.add_trace(go.Scattermapbox(\n",
    "        lat=np.array(polepredgroup_final)[:,0],\n",
    "        lon=np.array(polepredgroup_final)[:,1],\n",
    "        name='Predicted pole locations',\n",
    "        mode='markers',\n",
    "        marker=go.scattermapbox.Marker(\n",
    "            size=10,\n",
    "            color='red',\n",
    "            opacity=0.9\n",
    "        )))\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}